{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlpkvIja0Ng5",
        "outputId": "443b9859-20e2-42ee-d601-070d1282b69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy.stats import randint, uniform\n",
        "import warnings\n",
        "\n",
        "# --- Importing Classifiers ---\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# --- Configuration ---\n",
        "# Suppress future warnings for a cleaner output\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "print(\"--- Step 1: Loading and Preprocessing Data ---\")\n",
        "try:\n",
        "    # Load the dataset from the specified path.\n",
        "    # low_memory=False is added to address the DtypeWarning, though the cleaning step below is the primary fix.\n",
        "    df = pd.read_csv(\"features_filtered.csv\", low_memory=False)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'features_filtered.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "# Separate features (X) from the target variable (y).\n",
        "# Dropping metadata columns that are not predictive features.\n",
        "X = df.drop(columns=['file', 'run', 'label', 'onset_s'])\n",
        "y = df['label']\n",
        "\n",
        "# --- Data Cleaning Step ---\n",
        "# The DtypeWarning and ValueError indicate that some columns may contain non-numeric\n",
        "# data (like truncated numbers). We will iterate through all feature columns,\n",
        "# attempt to convert them to a numeric type, and replace any values that fail\n",
        "# conversion with the column's median.\n",
        "print(\"Cleaning non-numeric data from feature set...\")\n",
        "for col in X.columns:\n",
        "    # 'coerce' will turn any non-numeric values into NaN (Not a Number)\n",
        "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "    # Check if any NaNs were created during coercion\n",
        "    if X[col].isnull().any():\n",
        "        # Calculate the median of the column, ignoring NaN values\n",
        "        median_val = X[col].median()\n",
        "        # Fill any NaN values with the median. `inplace=True` modifies the DataFrame directly.\n",
        "        X[col].fillna(median_val, inplace=True)\n",
        "print(\"Data cleaning complete.\")\n",
        "\n",
        "\n",
        "# Encode the categorical target variable 'label' into numerical format.\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets (70% train, 30% test).\n",
        "# 'stratify=y_encoded' ensures that the class distribution is the same in both train and test sets,\n",
        "# which is crucial for imbalanced datasets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Scale the features using StandardScaler. This standardizes features by removing the mean\n",
        "# and scaling to unit variance. It's essential for distance-based algorithms like SVM and MLP.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data Loading and Preprocessing Complete.\\n\")\n",
        "\n",
        "\n",
        "# --- 2. Model and Hyperparameter Space Definition ---\n",
        "print(\"--- Step 2: Defining Models and Hyperparameter Search Spaces ---\")\n",
        "\n",
        "# A dictionary to hold the models and their corresponding hyperparameter distributions for tuning.\n",
        "models_to_tune = {}\n",
        "\n",
        "# 2.1 Support Vector Machine (SVM)\n",
        "svm_params = {'C': uniform(0.1, 10), 'gamma': uniform(0.001, 0.1), 'kernel': ['rbf']}\n",
        "models_to_tune['SVM'] = (SVC(probability=True, random_state=42), svm_params)\n",
        "\n",
        "# 2.2 Decision Tree\n",
        "dt_params = {'max_depth': randint(5, 50), 'min_samples_split': randint(2, 20), 'min_samples_leaf': randint(1, 20), 'criterion': ['gini', 'entropy']}\n",
        "models_to_tune['Decision Tree'] = (DecisionTreeClassifier(random_state=42), dt_params)\n",
        "\n",
        "# 2.3 Random Forest\n",
        "rf_params = {'n_estimators': randint(100, 500), 'max_depth': randint(10, 100), 'min_samples_split': randint(2, 20), 'min_samples_leaf': randint(1, 20)}\n",
        "models_to_tune['Random Forest'] = (RandomForestClassifier(random_state=42, n_jobs=-1), rf_params)\n",
        "\n",
        "# 2.4 AdaBoost\n",
        "ada_params = {'n_estimators': randint(50, 500), 'learning_rate': uniform(0.01, 1.0)}\n",
        "models_to_tune['AdaBoost'] = (AdaBoostClassifier(random_state=42), ada_params)\n",
        "\n",
        "# 2.5 CatBoost\n",
        "cat_params = {'iterations': randint(100, 500), 'learning_rate': uniform(0.01, 0.3), 'depth': randint(4, 10), 'l2_leaf_reg': uniform(1, 10)}\n",
        "models_to_tune['CatBoost'] = (CatBoostClassifier(random_state=42, silent=True), cat_params)\n",
        "\n",
        "# 2.6 XGBoost\n",
        "xgb_params = {'n_estimators': randint(100, 500), 'max_depth': randint(3, 10), 'learning_rate': uniform(0.01, 0.3)}\n",
        "models_to_tune['XGBoost'] = (XGBClassifier(eval_metric='mlogloss', random_state=42), xgb_params)\n",
        "\n",
        "# 2.7 Gaussian Naive Bayes\n",
        "# Naive Bayes has fewer hyperparameters to tune. 'var_smoothing' is a stability parameter.\n",
        "gnb_params = {'var_smoothing': uniform(1e-10, 1e-7)}\n",
        "models_to_tune['Gaussian NB'] = (GaussianNB(), gnb_params)\n",
        "\n",
        "# 2.8 MLP Classifier (Neural Network)\n",
        "# Added early_stopping for efficiency: training stops when validation score is not improving.\n",
        "mlp_params = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['tanh', 'relu'], 'alpha': uniform(0.0001, 0.01), 'learning_rate_init': uniform(0.001, 0.1)}\n",
        "models_to_tune['MLP Classifier'] = (MLPClassifier(random_state=42, max_iter=500, early_stopping=True, n_iter_no_change=10), mlp_params)\n",
        "\n",
        "print(\"Model definitions are ready.\\n\")\n",
        "\n",
        "\n",
        "# --- 3. Hyperparameter Tuning using RandomizedSearchCV ---\n",
        "print(\"--- Step 3: Performing Hyperparameter Tuning ---\")\n",
        "\n",
        "best_models = {}\n",
        "for name, (model, params) in models_to_tune.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "    # RandomizedSearchCV is generally faster than GridSearchCV and explores a wider range of hyperparameters.\n",
        "    # n_iter: Number of parameter settings that are sampled. Trades off runtime vs. quality of the solution.\n",
        "    # cv: Number of cross-validation folds. 3 is faster, 5 is more robust.\n",
        "    # n_jobs=-1: Use all available CPU cores to speed up the process.\n",
        "    random_search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_distributions=params,\n",
        "        n_iter=20, # Can be increased for more thorough search, or decreased for speed\n",
        "        cv=3,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train_scaled, y_train)\n",
        "    best_models[name] = random_search.best_estimator_\n",
        "    print(f\"✓ {name} tuning complete. Best score: {random_search.best_score_:.4f}\\n\")\n",
        "\n",
        "\n",
        "# --- 4. Model Evaluation ---\n",
        "print(\"--- Step 4: Evaluating All Tuned Models ---\")\n",
        "\n",
        "results = {}\n",
        "for name, model in best_models.items():\n",
        "    # Predictions on the training set to check for overfitting\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "\n",
        "    # Predictions on the unseen test set to evaluate generalization\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Store metrics for the training set\n",
        "    results[f\"{name}_Train\"] = {\n",
        "        'Accuracy': accuracy_score(y_train, y_train_pred),\n",
        "        'Precision': precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
        "        'Recall': recall_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
        "        'F1-score': f1_score(y_train, y_train_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "    # Store metrics for the test set\n",
        "    results[f\"{name}_Test\"] = {\n",
        "        'Accuracy': accuracy_score(y_test, y_test_pred),\n",
        "        'Precision': precision_score(y_test, y_test_pred, average='macro', zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_test_pred, average='macro', zero_division=0),\n",
        "        'F1-score': f1_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "print(\"Evaluation complete.\\n\")\n",
        "\n",
        "# --- 5. Final Performance Report ---\n",
        "# Create a pandas DataFrame from the results dictionary for clean tabulation.\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "\n",
        "# Create a MultiIndex for better organization, separating Model from Dataset (Train/Test).\n",
        "results_df.index = pd.MultiIndex.from_tuples(\n",
        "    [(name.split('_')[0], name.split('_')[1]) for name in results_df.index],\n",
        "    names=['Model', 'Dataset']\n",
        ")\n",
        "\n",
        "print(\"--- COMPREHENSIVE PERFORMANCE REPORT ---\")\n",
        "print(\"The table below compares the performance of each model on both the training and test datasets.\")\n",
        "print(\"Observations to make:\")\n",
        "print(\"1. High Train score and low Test score indicates overfitting.\")\n",
        "print(\"2. Similar scores on both sets suggest a well-generalized model.\")\n",
        "print(\"------------------------------------------\\n\")\n",
        "# Display the final results, rounded to 3 decimal places for readability.\n",
        "print(results_df.round(3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M59T4RQU1dbN",
        "outputId": "cd044f0e-f027-4688-822d-0bb9431334d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Loading and Preprocessing Data ---\n",
            "Cleaning non-numeric data from feature set...\n",
            "Data cleaning complete.\n",
            "Data Loading and Preprocessing Complete.\n",
            "\n",
            "--- Step 2: Defining Models and Hyperparameter Search Spaces ---\n",
            "Model definitions are ready.\n",
            "\n",
            "--- Step 3: Performing Hyperparameter Tuning ---\n",
            "Tuning SVM...\n",
            "✓ SVM tuning complete. Best score: 0.5850\n",
            "\n",
            "Tuning Decision Tree...\n",
            "✓ Decision Tree tuning complete. Best score: 0.5126\n",
            "\n",
            "Tuning Random Forest...\n",
            "✓ Random Forest tuning complete. Best score: 0.5493\n",
            "\n",
            "Tuning AdaBoost...\n",
            "✓ AdaBoost tuning complete. Best score: 0.5310\n",
            "\n",
            "Tuning CatBoost...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3y4jWoIT1cIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}