{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HIUeH_Uox6x",
        "outputId": "ce15f7de-ea58-4cf7-f5d1-ff3eecc70824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from scipy.stats import randint, uniform  # Utilities for defining hyperparameter search spaces\n",
        "\n",
        "\n",
        "# Loading and preprocess data\n",
        "df = pd.read_csv(r'/content/final_min_features_filtered.csv')  # Load dataset\n",
        "X = df.drop(columns=['file', 'run', 'label', 'onset_s'])       # Drop non-feature columns\n",
        "y = df['label']\n",
        "\n",
        "# Encode string labels into numeric values\n",
        "#label encoder is ued when you target labels y are categorical string or non numeric\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)#fit mapping unique labels  and #transform replaces each label y with its corresponding integer\n",
        "\n",
        "# Train-test split with stratification to maintain class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")#y_encoded is target variable(encoded labels) and stratify to keep them 30% for test and other for training same in original dataset\n",
        "\n",
        "# Standardize features (important for models like SVM, MLP)\n",
        "#age in year,weight inkgs so we need standardiztion so keep mean =0 and standard deviation=1\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data Preprocessing Complete.\\n\")\n",
        "\n",
        "\n",
        "# Define models and hyperparameter spaces\n",
        "models_to_tune = {}\n",
        "#allows to call pedict.probability\n",
        "models_to_tune['SVM'] = (\n",
        "    SVC(probability=True, random_state=42),\n",
        "    {'C': uniform(0.1, 10), 'gamma': uniform(0.001, 0.1)}\n",
        ")#c is a randomizedsearchcV,controls trade off simple decision boundary and training points\n",
        "\n",
        "models_to_tune['Decision Tree'] = (\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    {'max_depth': randint(5, 50)}#for decision tree  and random 5 to 49\n",
        ")\n",
        "\n",
        "models_to_tune['Random Forest'] = (\n",
        "    RandomForestClassifier(random_state=42, n_jobs=-1),#n_jobs=-1 use all available cores\n",
        "    {'n_estimators': randint(100, 200), 'max_depth': randint(10, 50)}#trained in parallel\n",
        "    #random forest uses more decision tress ,more tree more accurate\n",
        ")\n",
        "#train decision tree one after other and new focus on old learned mistake\n",
        "#all learned are combined to make prediction\n",
        "models_to_tune['AdaBoost'] = (\n",
        "    AdaBoostClassifier(random_state=42),\n",
        "    {'n_estimators': randint(50, 200), 'learning_rate': uniform(0.01, 1.0)}\n",
        ")#choose random as small lead to more estmators and larger lead to overfitiing\n",
        "\n",
        "models_to_tune['CatBoost'] = (\n",
        "    CatBoostClassifier(random_state=42, silent=True),#silent=true for reducing execution time as it give sbest score,loss,more content\n",
        "    {'iterations': randint(100, 200), 'learning_rate': uniform(0.01, 0.3)}\n",
        ")\n",
        "\n",
        "models_to_tune['XGBoost'] = (#lower mlogless better model and it is multi class logarithmic loss,used for classification\n",
        "    XGBClassifier(eval_metric='mlogloss', random_state=42, use_label_encoder=False),#use_label to avoid warning\n",
        "    {'n_estimators': randint(100, 200), 'max_depth': randint(3, 10)}\n",
        ")#It penalizes incorrect predictions, especially if the model is very confident but wrong.\n",
        "\n",
        "models_to_tune['Gaussian NB'] = (\n",
        "    GaussianNB(),#naive bayes is a probabilistic classifier based on bayes theorem\n",
        "    {'var_smoothing': uniform(1e-10, 1e-7)}#a fetaure can have zero variance divison by zero to avoid variance smoothing\n",
        ")\n",
        "\n",
        "models_to_tune['MLP Classifier'] = (\n",
        "    MLPClassifier(random_state=42, max_iter=500),\n",
        "    {'hidden_layer_sizes': [(50,), (100,)], 'alpha': uniform(0.0001, 0.01)}#reduces overftting not grow weights too high\n",
        ")\n",
        "#one hidden layer 50 neurons ,one hidden layer with 100 neurons\n",
        "\n",
        "# Hyperparameter tuning with RandomizedSearchCV\n",
        "best_models = {}\n",
        "for name, (model, params) in models_to_tune.items():\n",
        "    print(f\"Rapidly tuning {name}...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_distributions=params,\n",
        "        n_iter=5,#only test 5 random combinations of parameters\n",
        "        cv=2,#training set split into 2 parts\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train_scaled, y_train)\n",
        "    best_models[name] = random_search.best_estimator_\n",
        "    print(f\"{name} tuning complete.\")\n",
        "\n",
        "\n",
        "#  Evaluate tuned models on train & test sets\n",
        "print(\"\\nStep 3: Evaluating All Tuned Models...\")\n",
        "results = {}\n",
        "\n",
        "for name, model in best_models.items():\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    results[f\"{name}_Train\"] = {\n",
        "        'Accuracy': accuracy_score(y_train, y_train_pred),\n",
        "        'F1-score': f1_score(y_train, y_train_pred, average='macro')#macro is used for suppose A class 95%,B CLASS 5% ALL CLASS A\n",
        "    }\n",
        "    results[f\"{name}_Test\"] = {\n",
        "        'Accuracy': accuracy_score(y_test, y_test_pred),\n",
        "        'F1-score': f1_score(y_test, y_test_pred, average='macro')\n",
        "    }\n",
        "\n",
        "\n",
        "# Organize results in a clean table\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')#to convert results into a pandas dataframe\n",
        "results_df.index = pd.MultiIndex.from_tuples(\n",
        "    [(name.split('_')[0], name.split('_')[1]) for name in results_df.index],#to split svm_train into \"svm\",\"train\"\n",
        "    names=['Model', 'Dataset']\n",
        ")\n",
        "\n",
        "print(\"\\nRAPID PERFORMANCE REPORT\")\n",
        "print(results_df.round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvBP_Wn_rGHR",
        "outputId": "ac63811e-dc51-4622-8256-ea89f7c64257"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Preprocessing Complete.\n",
            "\n",
            "Rapidly tuning SVM...\n",
            "SVM tuning complete.\n",
            "Rapidly tuning Decision Tree...\n",
            "Decision Tree tuning complete.\n",
            "Rapidly tuning Random Forest...\n",
            "Random Forest tuning complete.\n",
            "Rapidly tuning AdaBoost...\n",
            "AdaBoost tuning complete.\n",
            "Rapidly tuning CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost tuning complete.\n",
            "Rapidly tuning XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:02:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost tuning complete.\n",
            "Rapidly tuning Gaussian NB...\n",
            "Gaussian NB tuning complete.\n",
            "Rapidly tuning MLP Classifier...\n",
            "MLP Classifier tuning complete.\n",
            "\n",
            "Step 3: Evaluating All Tuned Models...\n",
            "\n",
            "RAPID PERFORMANCE REPORT\n",
            "                        Accuracy  F1-score\n",
            "Model          Dataset                    \n",
            "SVM            Train       0.964     0.951\n",
            "               Test        0.544     0.340\n",
            "Decision Tree  Train       0.849     0.812\n",
            "               Test        0.468     0.305\n",
            "Random Forest  Train       1.000     1.000\n",
            "               Test        0.553     0.326\n",
            "AdaBoost       Train       0.539     0.253\n",
            "               Test        0.523     0.223\n",
            "CatBoost       Train       0.886     0.874\n",
            "               Test        0.556     0.345\n",
            "XGBoost        Train       1.000     1.000\n",
            "               Test        0.566     0.386\n",
            "Gaussian NB    Train       0.238     0.194\n",
            "               Test        0.231     0.196\n",
            "MLP Classifier Train       0.862     0.816\n",
            "               Test        0.547     0.423\n"
          ]
        }
      ]
    }
  ]
}